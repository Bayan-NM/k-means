# -*- coding: utf-8 -*-
"""kmeans-task4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qDSf-oBA1SrW7WqawUeNTT0dqL6dnuQg

###import library
"""

import numpy as np

"""
# 1. Load word embeddings from GloVe file"""

def load_glove_embeddings(filename):
    # create an empty dictionary to store the word embeddings
    embeddings = {}
    # open the file and read each line
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            # split the line by whitespace
            tokens = line.split()
            # get the word and the embedding vector
            word = tokens[0]
            vector = np.array(tokens[1:], dtype='float32')
            # store the word and the vector in the dictionary
            embeddings[word] = vector
    # return the dictionary of word embeddings
    return embeddings

"""#2. Load EVALution Dataset"""

def load_evalution_dataset(file):
    dataset = []
    classes =[]
    train_instances = {}
    with open(file, 'r', encoding='utf-8') as f:
        for line in f:
            word_1, word_2,relation_type = line.strip().split('\t')
            dataset.append(((word_1, word_2), relation_type))
            classes.append(relation_type)

            if relation_type not in train_instances: 
               train_instances[relation_type] = 0 
            train_instances[relation_type] += 1

    # print the statistics 
    num_classes = len(classes)
    num_train_instances = len(train_instances)
    print("Number of classes", num_classes) 
    print("Number of training instances per class:" )
    for relation_type, count in train_instances.items():
        print(relation_type, ":", count)
    
    return dataset

"""# 3. Compute word pair vectors"""

def get_offset_vector(dataset, embeddings):
    offsets = []
    label =[]
    for item in dataset:
        word_pair, rel_type = item
        w1, w2 = word_pair
        if w1 in embeddings and w2 in embeddings:
            offset = embeddings[w2] - embeddings[w1]
            offsets.append(offset)
            label.append(rel_type)
    return np.array(offsets), label

"""#To implement the k-means clustering algorithm with Euclidean distance"""

#Define a function to compute the Euclidean distance between two vectors:

def euclidean_distance(vec1, vec2):
    # use numpy to calculate and return the Euclidean distance
    return np.linalg.norm(vec1 - vec2)
#Define a function to initialize k random centroids from the data:
def initialize_centroids(data, k):
    # get the number of features in the data
    n_features = data.shape[1]
    # create an empty array to store the centroids
    centroids = np.zeros((k, n_features))
    # for each cluster
    for i in range(k):
        # randomly choose an index from the data
        index = np.random.choice(data.shape[0])
        # assign the corresponding data point as the centroid
        centroids[i] = data[index]
    # return the centroids array
    return centroids

def assign_clusters(data, centroids):
    # get the number of data points and clusters
    n_data = data.shape[0]
    n_clusters = centroids.shape[0]
    # create an empty array to store the cluster assignments
    clusters = np.zeros(n_data)
    # for each data point
    for i in range(n_data):
        # initialize the minimum distance and cluster index as infinity and -1 respectively
        min_dist = np.inf
        cluster_index = -1
        # for each cluster centroid
        for j in range(n_clusters):
            # compute the Euclidean distance between the data point and the centroid
            dist = euclidean_distance(data[i], centroids[j])
            # if the distance is smaller than the minimum distance so far
            if dist < min_dist:
                # update the minimum distance and cluster index accordingly
                min_dist = dist
                cluster_index = j
        # assign the data point to its nearest cluster index 
        clusters[i] = cluster_index 
    # return the cluster assignments array 
    return clusters 

def update_centroids(data, clusters, k):
    # get the number of features in the data
    n_features = data.shape[1]
    # create an empty array to store the new centroids
    new_centroids = np.zeros((k, n_features))
    # for each cluster
    for i in range(k):
        # get the indices of the data points that belong to that cluster
        indices = np.where(clusters == i)
        # get the subset of the data that belong to that cluster
        cluster_data = data[indices]
        # compute and assign the mean of the cluster data as the new centroid
        new_centroids[i] = np.mean(cluster_data, axis=0)
    # return the new centroids array
    return new_centroids

def kmeans(data, k, max_iter=100):
    # initialize the centroids randomly
    centroids = initialize_centroids(data, k)
    # create a variable to store the previous cluster assignments
    prev_clusters = None
    # create a variable to store the current number of iterations
    n_iter = 0
    # loop until convergence or maximum number of iterations
    while True:
        # assign clusters based on the current centroids
        clusters = assign_clusters(data, centroids)
        # update centroids based on the current clusters
        centroids = update_centroids(data, clusters, k)
        # increment the number of iterations
        n_iter += 1
        # check if the cluster assignments have changed or reached the maximum number of iterations
        if np.array_equal(clusters, prev_clusters) or n_iter >= max_iter:
            # break the loop
            break
        # otherwise, date the previous cluster assignments with the current ones
        prev_clusters = clusters
    # return the final cluster assignments 
    return clusters

"""
# 5. Compute precision, recall, and F-score"""

from sklearn.metrics import precision_score, recall_score, f1_score

def compute_metrics(clusters, labels):
    precision = precision_score(labels, clusters, average='macro')
    recall = recall_score(labels, clusters, average='macro')
    fscore = f1_score(labels, clusters, average='macro')

    return precision, recall, fscore

"""# 6. Plot k in the horizontal axis and precision, recall, and F-score in the vertical axis"""

import matplotlib.pyplot as plt

def plot(ks, precisions, recalls, fscores):
    plt.plot(ks, precisions, label='Precision')
    plt.plot(ks, recalls, label='Recall')
    plt.plot(ks, fscores, label='F-score')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Score')
    plt.title('K-means clustering metrics')
    plt.legend()
    plt.show()

"""#normalize"""

import numpy as np

def normalize_vectors(vectors):
    # Compute vector norms (magnitudes)
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    # Avoid division by zero
    norms[norms == 0] = 1
    # Divide vectors by their norms
    normalized_vectors = vectors / norms
    return normalized_vectors

def convert_labels_to_numeric(labels):
    unique_labels = sorted(set(labels))
    label_mapping = {label: index for index, label in enumerate(unique_labels)}
    numeric_labels = [label_mapping[label] for label in labels]
    return numeric_labels

"""#Test"""

# Load word embeddings
embeddings = load_glove_embeddings('glove.6B.50d.txt')
# Load EVALution dataset
dataset = load_evalution_dataset('train.tsv')
# Compute word pair vectors
offset_vectors,relation_types =get_offset_vector(dataset,embeddings)
# Convert  relation_types to numeric format
relation_types= convert_labels_to_numeric(relation_types)

k_values = range(1, 11)
precisions = [] 
recalls = [] 
fscores = []
# Run k-means and compute metrics for different numbers of clusters
for k in k_values:
    clusters = kmeans(offset_vectors, k)
    #predicted_labels = [label [lab] for lab in predicted_labels]    
    precision, recall, fscore = compute_metrics(clusters,relation_types)
    precisions.append(precision) 
    recalls.append(recall) 
    fscores.append(fscore)
# Plot 
plot(k_values,precisions, recalls, fscores)

# normalize the offset vectors
offset_vectors = normalize_vectors(offset_vectors)

k_values = range(1, 11)

precisions = [] 
recalls = [] 
fscores = []
# Run k-means and compute metrics for different numbers of clusters
for k in k_values:
    clusters = kmeans(offset_vectors, k)
    #predicted_labels = [label [lab] for lab in predicted_labels]    
    precision, recall, fscore = compute_metrics(clusters,relation_types)
    precisions.append(precision) 
    recalls.append(recall) 
    fscores.append(fscore)
# Plot 
plot(k_values,precisions, recalls, fscores)