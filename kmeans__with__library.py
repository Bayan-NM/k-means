# -*- coding: utf-8 -*-
"""kmeans _with_ library.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYy08cw--Q80VSJHP-ILnsmWsfT_RW95
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize

# Task 1: Load word embeddings from the GloVe file
def load_word_embeddings(filename):
    word_embeddings = {}
    with open(filename, 'r') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], dtype='float32')
            word_embeddings[word] = vector
    return word_embeddings

word_embeddings = load_word_embeddings('/content/glove.6B.50d.txt')

# Task 2: Load train.tsv and split into training and testing sets
train_data = pd.read_csv('/content/train.tsv', sep='\t', header=None, names=['word1', 'word2', 'relation'])
num_classes = len(train_data['relation'].unique())
train_size_per_class = train_data.groupby('relation').size().values.tolist()

# Task 3: Calculate vector offsets for each word pair in the training set
def calculate_vector_offsets(dataset, word_embeddings):
    vector_offsets = []
    for index, row in dataset.iterrows():
        word1 = row['word1']
        word2 = row['word2']
        vector_offset = word_embeddings.get(word2, np.zeros(50)) - word_embeddings.get(word1, np.zeros(50))
        vector_offsets.append(vector_offset)
    return np.asarray(vector_offsets)

train_vector_offsets = calculate_vector_offsets(train_data, word_embeddings)
# Print the statistics of the dataset
print('Number of classes:', len(train_counts))
print('Relation types:', train_counts.index.tolist())
print('Number of training instances per class:')
train_counts
# Implement k-means clustering algorithm
def kmeans_clustering(X, k):
    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)
    return kmeans.labels_

# Cluster word pairs into k clusters and compute precision, recall, and F-score
k_values = range(1, 11)
precisions = []
recalls = []
f_scores = []
for k in k_values:
    labels = kmeans_clustering(train_vector_offsets, k)
    clusters = [train_data[labels == i] for i in range(k)]
    pred_relations = []
    true_relations = []
    for i, cluster in enumerate(clusters):
        if len(cluster) == 0:
            continue
        pred_relation = cluster['relation'].value_counts().index[0]
        true_relation = cluster['relation'].values[0]
        pred_relations += [pred_relation] * len(cluster)
        true_relations += [true_relation] * len(cluster)
    precision = precision_score(true_relations, pred_relations, average='macro')
    recall = recall_score(true_relations, pred_relations, average='macro')
    f_score = f1_score(true_relations, pred_relations, average='macro')
    precisions.append(precision)
    recalls.append(recall)
    f_scores.append(f_score)

# Plot precision, recall, and F-score as a function of k
plt.plot(k_values, precisions, label='Precision')
plt.plot(k_values, recalls, label='Recall')
plt.plot(k_values, f_scores, label='F-score')
plt.xlabel('k')
plt.ylabel('Score')
plt.title('Clustering performance')
plt.legend()
plt.show()

# Normalize feature vectors and repeat clustering and evaluation
train_vector_offsets_norm = normalize(train_vector_offsets)
precisions_norm = []
recalls_norm = []
f_scores_norm = []
for k in k_values:
    labels = kmeans_clustering(train_vector_offsets_norm, k)
    clusters = [train_data[labels == i] for i in range(k)]
    pred_relations = []
    true_relations = []
    for i, cluster in enumerate(clusters):
        if len(cluster) == 0:
            continue
        pred_relation = cluster['relation'].value_counts().index[0]
        true_relation = cluster['relation'].values[0]
        pred_relations += [pred_relation] * len(cluster)
        true_relations += [true_relation] * len(cluster)
    precision = precision_score(true_relations, pred_relations, average='macro')
    recall = recall_score(true_relations, pred_relations, average='macro')
    f_score = f1_score(true_relations, pred_relations, average='macro')
    precisions_norm.append(precision)
    recalls_norm.append(recall)
    f_scores_norm.append(f_score)

# Plot precision, recall, and F-score as a function of k (with normalization)
plt.plot(k_values, precisions_norm, label='Precision')
plt.plot(k_values, recalls_norm, label='Recall')
plt.plot(k_values, f_scores_norm, label='F-score')
plt.xlabel('k')
plt.ylabel('Score')
plt.title('Clustering performance after normalization')
plt.legend()
plt.show()